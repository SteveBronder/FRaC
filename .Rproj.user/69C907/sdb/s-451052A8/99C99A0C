{
    "contents" : "#' FRaK: Feature Modeling Approach to Anomaly Detection\n#'\n#'@author Steve Bronder\n#'@description The FRaC Algorithm\n#'@usage frac(x,  models, n.cv=2, keys = NULL grid=NULL, allowParallel = FALSE, tuneList = NULL)\n#'\n#'@details Version 0.0.3 Alpha\n#'\n#'@param x An NxT matrix with T observations of N varialbles\n#'\n#'@param keys A vector of integers representing places that the data set has time variables. This is only here for short term bug fixes\n#'\n#'@param tuneList A list of models for train functions \n#'\n#'@param models a string vector of models available in the caret package\n#'\n#'@param n.cv An integer specifying the number of cross-validations\n#'\n#'@param grid A list containing the different parameter values for each models iterations. Can be set to default NULL\n#'\n#'@param allowParallel Logical definining whether or not the user would like to use a parallel backend if one is set up\n#'\n#'@return values or sup: the normalised suprisal score for each observation. Higher scores equate to a higher chance of an observation being an outlier\n#'\n#'@references K. Noto, C. E. Brodley, and D. Slonim. \n#'FRaC: A Feature-Modeling Appraoch for Semi-Supervised and Unsupervised Anomaly Detection. \n#'Data Mining and Knowledge Discovery, 25(1), pp.109â€”133, 2011.\n#'\nfrac <- function(x, models = c(\"C5.0\",\"glm\", \"rf\",\"pls\"),keys = NULL, n.cv=10, allowParallel = FALSE, tuneList=NULL, ...){\n  # make X into a matrix\n  # Having the time variables come in made everything all goofy doofy? You can set them as keys\n  # So they are not the y, but will allows them to be predictors.\n  if (!is.null(keys)){\n    xKeys <- x[,keys]\n    \n    x <- as.data.frame(x[,-keys])\n    \n  }else{\n  x <- as.data.frame(x)\n  }\n  \n  \n  # gather data length and width\n  D <- dim(x)[2]\n  N <- dim(x)[1]\n  \n  #number of bins to be used later\n  sq.N <- as.integer(ceiling(sqrt(N)))\n  \n  # Create the variable for the probability distribution of each error term\n  AllErrorProbs <- NULL\n  \n  # Classify out which models are regresssions\n  RegModels <-  which(\n    unlist(\n      sapply(models, function(k){\n        modelLookup(k)$forReg[1]\n      })\n    ) == \"TRUE\"\n  )\n  \n  TuneRegModels <-  which(\n    unlist(\n      sapply(1:length(tuneList), function(k){\n        modelLookup(tuneList[[k]][[1]])$forReg[1]\n      })\n    ) == \"TRUE\"\n  )\n  \n  \n  # for i in one to number of features\n  ErrorProbability <- lapply(1:D, function(i) {\n    \n    if (!is.null(keys)){\n      y <- x[,i]\n      \n      x.m <- cbind(x[-i], xKeys)\n      \n    }else{\n    # Grab y variable\n    y <- x[,i]\n    \n    # grab rest of the matrix\n    x.m <- x[,-i]\n    }\n    # Controls for caret\n    controls <- trainControl(method = \"cv\",\n                             number=n.cv,\n                             savePredictions=TRUE,\n                             index = createFolds(y,k=n.cv,returnTrain=TRUE),\n                             allowParallel = allowParallel)\n    \n    # If the y variable is numeric then cut out\n    # all the classification models and vice versa\n    if (is.numeric(y) == \"TRUE\"){\n      ModelToDo <- models[RegModels]\n      TuneToDo <- tuneList[TuneRegModels]\n      \n    }else{\n      ModelToDo <- models[-RegModels]\n      TuneToDo <- tuneList[-TuneRegModels]\n    }\n    \n    #Do models together\n    TrainMethod <- suppressMessages(suppressWarnings(caretList(y=y,x=x.m,\n                                              methodList=ModelToDo,\n                                              trControl = controls,\n                             tuneList=TuneToDo)))\n    \n    \n    PredClass <- predict(TrainMethod,x.m)\n    \n    if (is.numeric(y) != \"TRUE\") PredClass <- as.data.frame(factor(PredClass,levels=levels(y),labels=levels(y)))\n    # make data frame of errors and row numbers\n    #      PredClass <- do.call(cbind, lapply(1:length(ModelToDo),function(i){\n    #        if (TrainMethod[[i]]$bestTune[1] == \"none\"){\n    #          \n    #          PreSort <- TrainMethod[[i]]$pred[,c(\"pred\",\"rowIndex\")]\n    #    \n    #          PostSort <- PreSort[ order(PreSort[,2]),1]\n    #    \n    #         \n    #        }else{\n    #          # Really wacky function to get find best resampling parameters\n    #          PreSort <- TrainMethod[[i]]$pred[\n    #              which(TrainMethod[[i]]$pred[\n    #                colnames(TrainMethod[[i]]$bestTune)] = TrainMethod[[i]]$bestTune),\n    #            c(\"pred\",\"rowIndex\")\n    #            ]\n    #           \n    #          PostSort <- PreSort[ order(PreSort[,2]),1]\n    #          \n    #        }\n    #      }\n    #      )\n    #      )\n    # Set Numeric and Class Probs\n    NumericProbs <- NULL\n    ClassProbs <- NULL\n    \n    if (is.numeric(y) == \"TRUE\"){\n      \n      # Begin an lapply to go over the predictions from each model\n      NumericProbs <- lapply(1:I(length(ModelToDo)+length(TuneToDo)), function(j){\n        \n        # make the error term\n        Error <- y - PredClass[,j]\n        \n        # Make a data frame that consists of bins (key) and obs for sorting later\n        ErrorBins <- data.frame( key = cut(Error,sq.N,labels=FALSE), obs = 1:N)\n        \n        # Find the density of each bin like in FRaC\n        ErrorDensity <- density(ErrorBins$key,n=sq.N,kernel = \"gaussian\")$y\n        \n        # Make a data frame for containing bins (key) and the density of each bin\n        ErrorDensityKey <- data.frame(key = 1:sq.N, value = ErrorDensity)\n        \n        # Merge together the Bins for each observation with the density of each bin\n        NumericMerge <- merge(ErrorBins, ErrorDensityKey, \"key\" )[,c(\"obs\",\"value\")]\n        \n        # Order the newly merged data by the row observations, keeping only the value variable\n        NumericMerge <- as.matrix(NumericMerge[ do.call(order, NumericMerge),2])\n        \n        # change column name to correspond to each model\n        colnames(NumericMerge) <- paste0(ModelToDo[j],\"Probs\",i)\n        \n        return(NumericMerge)\n      })\n      return(NumericProbs)\n    }else{\n      \n      # Begin an apply to find class probabilities for each observation\n      ClassProbs <- lapply(1:I(length(ModelToDo)+length(TuneToDo)), function(j){\n        \n        \n        \n        # Make confusion matrix for each model\n        ConfuseTable <- confusionMatrix(PredClass[,j],y)$table\n        \n        # Get cound for each row of confusion table \n        RowCount <- apply(ConfuseTable,1,sum)\n        \n        # normalize the cound of each confusion table\n        # Noto adds 1 to each value, but I'm questionable about this?\n        ConfuseNorm <- (ConfuseTable + 1) / RowCount\n        \n        ConfuseNorm[which(ConfuseNorm==Inf)] <- 0.001\n        # Melt the data frame to get ready for merge\n        ConfuseMelt <- reshape2::melt(ConfuseNorm)\n        \n        # Create key of class/class and associated probability (Right Merger)\n        RightMerge<- data.frame(key = paste0(ConfuseMelt[,1], ConfuseMelt[,2]),value = ConfuseMelt$value)\n        \n        # Create a key of class/class and associated observation (Left Merger)\n        LeftMerge <- data.frame(key = paste0(as.character(y),as.character(PredClass[,j])),obs = 1:N)\n        \n        # Push through merge by key. Drops everything but observation and value\n        ConfuseMerge <- merge(LeftMerge, RightMerge, \"key\",all.x = TRUE, all.y =FALSE)[,c(\"obs\", \"value\")]\n        \n        # Sort the class proababilities by observation, leaving only the probs\n        ConfuseMerge <- as.matrix(ConfuseMerge[ do.call(order,ConfuseMerge),2])\n        \n        #make column name assocated with model\n        colnames(ConfuseMerge) <- paste0(ModelToDo[j],\"Probs\",i)\n        \n        return(ConfuseMerge)\n        \n      })\n      \n      return(ClassProbs)\n    }\n  }\n)\n  # ??????????????????\n  \n  AllErrorProbs<-do.call(cbind,as.data.frame(ErrorProbability))\n\n  #define suprisal for each model. Still need to account for NAs\n  Suprise <- data.frame( Suprise = apply(apply(AllErrorProbs,2,function(x) -log2(x)),1,sum), obs = 1:N)\n  \n  \n  # I'm not sure if this is necessary. On page 6 the definiton of entrophy starts getting strange...\n  ProportionsKey <- as.data.frame(table(Suprise$Suprise))\n  \n\n  ProportionsKey$Freq <- ProportionsKey$Freq / length(as.numeric(Suprise$Suprise))\n  \n  colnames(ProportionsKey) <- c(\"Suprise\", \"Freq\")\n  \n  ErrorProportions <- merge(Suprise,ProportionsKey,by = \"Suprise\")[,2:3]\n  \n  ErrorProportions <- as.matrix(ErrorProportions[ do.call(order,ErrorProportions),2])\n  \n  # define entrophy of models\n  Entropy <- sum(-ErrorProportions * log2(ErrorProportions))\n  \n  \n  norm.suprise <- Suprise$Suprise - Entropy\n  \n  \n  \n  values <- list(sup=norm.suprise)\n  \n  return(values)\n}\n",
    "created" : 1430519337450.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3814870840",
    "id" : "99C99A0C",
    "lastKnownWriteTime" : 1430519765,
    "path" : "C:/Users/brond_000/Documents/R/FRaC/R/frac.R",
    "project_path" : "R/frac.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}